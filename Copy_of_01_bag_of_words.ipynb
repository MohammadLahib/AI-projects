{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 01_bag_of_words",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2-lh/moh2-lh/blob/main/Copy_of_01_bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojIsNXa2r0JH"
      },
      "source": [
        "Â© 2021 Zaka AI, Inc. All Rights Reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD9Am8gAESrG"
      },
      "source": [
        "# Bag Of Words\n",
        "This notebook shows the most basic applications of the different functions used to create bag of words for texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RELNdcx9GPWp"
      },
      "source": [
        "\n",
        "### Word Counts\n",
        "> In this cell, we can see how the application of the the fit() and transform() functions enabled us to create a vocabulary of 8 words for the document. The vectorizer fit allowed us to build the vocab and the transform encoded it into number of appearances of each word. The indexing is done alphabetically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7KA4SZkEVL9"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycbj4ssz7cpP"
      },
      "source": [
        "Let's test on another example\n",
        "\n",
        "> In this example, we used the same vocabulary we built in the previous cell held in vectorizer but on a different text. Obviously the resulting count is different since the word 'brown' for example does not appear in this document. It is given the value 0, whereas the word 'quick' appears 3 times.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fOzu36W7eUj"
      },
      "source": [
        "text = [\"The quick quick quick fox jumped over a big dog\"]\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uePCWtn2EqW_"
      },
      "source": [
        "### Word frequencies\n",
        "> This section deals with the application of the TF-IDF formula in order to describe word appearances relative to multiple documents\n",
        "![TF-IDF formula](https://miro.medium.com/max/3604/1*ImQJjYGLq2GE4eX40Mh28Q.png)\n",
        "source: https://medium.com/nlpgurukool/tfidf-vectorizer-5421f1528402"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwGn0gREsdh"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\t\"The dog.\",\n",
        "\t\t\"The fox\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])\n",
        "\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxbH2D-MUJTk"
      },
      "source": [
        "> You can tell how, for example, the word 'brown' has a higher multiplier than 'the' eventhough the latter has two appearances in the first document. This is due to the fact 'the' appears in the other two documents which reduces its uniqueness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTQlNX_o7h1i"
      },
      "source": [
        "Let's test it on one other example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ujOO0ls7lMK"
      },
      "source": [
        "text = [\"The quick quick quick fox jumped over a big dog\"]\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHnhfrN_UwDY"
      },
      "source": [
        "> We find our selves with two null values because this document does not include the words 'brown' or 'lazy'. And since the word 'big' is not included in the vocab, it is discarded."
      ]
    }
  ]
}