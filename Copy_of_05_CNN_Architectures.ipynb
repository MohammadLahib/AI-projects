{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 05_CNN_Architectures",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2-lh/moh2-lh/blob/main/Copy_of_05_CNN_Architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-L3pbzW_ed"
      },
      "source": [
        "Â© 2021 Zaka AI, Inc. All Rights Reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhPDBQomxeoa"
      },
      "source": [
        "#CNN Architectures\n",
        "**Objective:** The goal from this exercise is to get familiar with the codes that built famous CNN models like AlexNet and VGG19. It also introduces three types of Transfer Learning Usages with pre-trained classifier, standalone feature extractor and pre-trained model with custom classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2okGaDmyzNL"
      },
      "source": [
        "## Model Architectures Build\n",
        "In this section, we will observe how the AlexNet and VGG models were built with code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewplb9gFPTr_"
      },
      "source": [
        "### AlexNet\n",
        "\n",
        "Code to build the AlexNet Architecture, winner of ILSVRC in 2012."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KH3LyfJPPYi"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "in_layer = layers.Input((227,227,3))\n",
        "conv1 = layers.Conv2D(96, 11, strides=4, activation='relu')(in_layer)\n",
        "pool1 = layers.MaxPool2D(3, 2)(conv1)\n",
        "conv2 = layers.Conv2D(256, 5, strides=1, padding='same', activation='relu')(pool1)\n",
        "pool2 = layers.MaxPool2D(3, 2)(conv2)\n",
        "conv3 = layers.Conv2D(384, 3, strides=1, padding='same', activation='relu')(pool2)\n",
        "conv4 = layers.Conv2D(256, 3, strides=1, padding='same', activation='relu')(conv3)\n",
        "pool3 = layers.MaxPool2D(3, 2)(conv4)\n",
        "flattened = layers.Flatten()(pool3)\n",
        "dense1 = layers.Dense(4096, activation='relu')(flattened)\n",
        "drop1 = layers.Dropout(0.5)(dense1)\n",
        "dense2 = layers.Dense(4096, activation='relu')(drop1)\n",
        "drop2 = layers.Dropout(0.5)(dense2)\n",
        "preds = layers.Dense(1000, activation='softmax')(drop2)\n",
        "\n",
        "model = Model(in_layer, preds)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer='sgd', metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8zyntPyQYhW"
      },
      "source": [
        "### VGG19\n",
        "Code to build the VGG16 and VGG19 Architectures, winner of ILSVRC in 2014."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqxzQKJuQd9d"
      },
      "source": [
        "# Code to build VGG Architecture\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "conv3 = partial(layers.Conv2D,\n",
        "                kernel_size=3,\n",
        "                strides=1,\n",
        "                padding='same',\n",
        "                activation='relu')\n",
        "\n",
        "def block(in_tensor, filters, n_convs):\n",
        "    conv_block = in_tensor\n",
        "    for _ in range(n_convs):\n",
        "        conv_block = conv3(filters=filters)(conv_block)\n",
        "    return conv_block\n",
        "\n",
        "def _vgg(in_shape=(224,224,3),\n",
        "         n_classes=1000,\n",
        "         opt='sgd',\n",
        "         n_stages_per_blocks=[2, 2, 3, 3, 3]):\n",
        "    in_layer = layers.Input(in_shape)\n",
        "\n",
        "    block1 = block(in_layer, 64, n_stages_per_blocks[0])\n",
        "    pool1 = layers.MaxPool2D()(block1)\n",
        "    block2 = block(pool1, 128, n_stages_per_blocks[1])\n",
        "    pool2 = layers.MaxPool2D()(block2)\n",
        "    block3 = block(pool2, 256, n_stages_per_blocks[2])\n",
        "    pool3 = layers.MaxPool2D()(block3)\n",
        "    block4 = block(pool3, 512, n_stages_per_blocks[3])\n",
        "    pool4 = layers.MaxPool2D()(block4)\n",
        "    block5 = block(pool4, 512, n_stages_per_blocks[4])\n",
        "    pool5 = layers.MaxPool2D()(block5)\n",
        "    flattened = layers.GlobalAvgPool2D()(pool5)\n",
        "  \n",
        "    # DNN\n",
        "    dense1 = layers.Dense(4096, activation='relu')(flattened)\n",
        "    dense2 = layers.Dense(4096, activation='relu')(dense1)\n",
        "    preds = layers.Dense(1000, activation='softmax')(dense2)\n",
        "\n",
        "    model = Model(in_layer, preds)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\t              metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def vgg16(in_shape=(224,224,3), n_classes=1000, opt='sgd'):\n",
        "    return _vgg(in_shape, n_classes, opt)\n",
        "\n",
        "def vgg19(in_shape=(224,224,3), n_classes=1000, opt='sgd'):\n",
        "    return _vgg(in_shape, n_classes, opt, [2, 2, 4, 4, 4])\n",
        "\n",
        "model = vgg19()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg0TZvRBbIb2"
      },
      "source": [
        "###Importing VGG19()\n",
        "An easier and better way to use VGG model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBPar1I8AhDN"
      },
      "source": [
        "# example of loading the vgg19 model\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "\n",
        "# load model\n",
        "model = VGG19()\n",
        "\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vniu95-4zE5D"
      },
      "source": [
        "##Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itG-EmNAE5ro"
      },
      "source": [
        "### Pretrained classifier\n",
        "\n",
        "How to use a pretrained classifier to directly predict on images. First, getting an image to work on using `curl`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpQdvtqGFEpy"
      },
      "source": [
        "# let's download an image from the internet\n",
        "!curl -o dog.jpg https://images.pexels.com/photos/356378/pexels-photo-356378.jpeg?auto=compress&cs=tinysrgb&dpr=1&w=500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBspBng_zW_f"
      },
      "source": [
        "Preparing the image and calling the VGG model to predict with it the class of the object in the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtxAB4KFE9XQ"
      },
      "source": [
        "# example of using a pre-trained model as a classifier\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "# load an image from file\n",
        "image = load_img('dog.jpg', target_size=(224, 224))\n",
        "\n",
        "# convert the image pixels to a numpy array\n",
        "image = img_to_array(image)\n",
        "\n",
        "# reshape data for the model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\n",
        "# prepare the image for the VGG model\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# load the model\n",
        "model = VGG16()\n",
        "\n",
        "# predict the probability across all output classes\n",
        "yhat = model.predict(image)\n",
        "\n",
        "# convert the probabilities to class labels\n",
        "label = decode_predictions(yhat)\n",
        "\n",
        "print(label)\n",
        "\n",
        "# retrieve the most likely result, e.g. highest probability\n",
        "label = label[0][0]\n",
        "\n",
        "# print the classification\n",
        "print('%s (%.2f%%)' % (label[1], label[2]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhLuOEcpGTVy"
      },
      "source": [
        "### Standalone Feature Extractor\n",
        "\n",
        "How to use the pretrained VGG model as a standalone feature extractor and using the same image from before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1zkUnwnGV0Z"
      },
      "source": [
        "# example of using the vgg16 model as a feature extraction model\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# load an image from file\n",
        "image = load_img('dog.jpg', target_size=(224, 224))\n",
        "\n",
        "# convert the image pixels to a numpy array\n",
        "image = img_to_array(image)\n",
        "\n",
        "# reshape data for the model\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\n",
        "# prepare the image for the VGG model\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# load model\n",
        "model = VGG16()\n",
        "\n",
        "# remove the output layer\n",
        "my_model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\n",
        "print(\"Passing image to model ...\")\n",
        "\n",
        "# get extracted features\n",
        "features = my_model.predict(image)\n",
        "print(\"Output shape: {}\".format(features.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBAPkmwUPvSB"
      },
      "source": [
        "### Integrated Feature Extractor\n",
        "We can also use the model architecture for feature extraction and integrate it into a new classification model. This means we load the CNN architecture without the fully connected layers and then build these ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2JfMM1ZHKiI"
      },
      "source": [
        "# example of tending the vgg16 model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "# load model without classifier layers\n",
        "model = Sequential()\n",
        "model.add(VGG16(include_top=False, input_shape=(300, 300, 3)))\n",
        "\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# summarize\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}