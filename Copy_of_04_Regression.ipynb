{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 04_Regression",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2-lh/moh2-lh/blob/main/Copy_of_04_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jlKITSzuNeA"
      },
      "source": [
        "Â© 2021 Zaka AI, Inc. All Rights Reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP0SbsvDaZ-b"
      },
      "source": [
        "#Regression\n",
        "**Objective:** In this notebook exercise, we will work in the Boston House Price dataset to predict through regression the price of houses in thousand of dollars. \n",
        "We will load the data, create a baseline model, train and evaluate it to predict with it and finally alter the performance of our model by standardizing our dataset and trying out wider and/or deeper network topologies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PH_opY2bazW"
      },
      "source": [
        "### Importing the data from the github repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwHKJzETO-yC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2362d469-428c-439d-9820-9fdd8596512d"
      },
      "source": [
        "# clone git repo\n",
        "!git clone https://github.com/zaka-ai/intro2dl.git\n",
        "\n",
        "# change directory\n",
        "%cd intro2dl/data/"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'intro2dl'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/16)\u001b[K\rremote: Counting objects:  12% (2/16)\u001b[K\rremote: Counting objects:  18% (3/16)\u001b[K\rremote: Counting objects:  25% (4/16)\u001b[K\rremote: Counting objects:  31% (5/16)\u001b[K\rremote: Counting objects:  37% (6/16)\u001b[K\rremote: Counting objects:  43% (7/16)\u001b[K\rremote: Counting objects:  50% (8/16)\u001b[K\rremote: Counting objects:  56% (9/16)\u001b[K\rremote: Counting objects:  62% (10/16)\u001b[K\rremote: Counting objects:  68% (11/16)\u001b[K\rremote: Counting objects:  75% (12/16)\u001b[K\rremote: Counting objects:  81% (13/16)\u001b[K\rremote: Counting objects:  87% (14/16)\u001b[K\rremote: Counting objects:  93% (15/16)\u001b[K\rremote: Counting objects: 100% (16/16)\u001b[K\rremote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects:   6% (1/15)\u001b[K\rremote: Compressing objects:  13% (2/15)\u001b[K\rremote: Compressing objects:  20% (3/15)\u001b[K\rremote: Compressing objects:  26% (4/15)\u001b[K\rremote: Compressing objects:  33% (5/15)\u001b[K\rremote: Compressing objects:  40% (6/15)\u001b[K\rremote: Compressing objects:  46% (7/15)\u001b[K\rremote: Compressing objects:  53% (8/15)\u001b[K\rremote: Compressing objects:  60% (9/15)\u001b[K\rremote: Compressing objects:  66% (10/15)\u001b[K\rremote: Compressing objects:  73% (11/15)\u001b[K\rremote: Compressing objects:  80% (12/15)\u001b[K\rremote: Compressing objects:  86% (13/15)\u001b[K\rremote: Compressing objects:  93% (14/15)\u001b[K\rremote: Compressing objects: 100% (15/15)\u001b[K\rremote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 16 (delta 1), reused 7 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:   6% (1/16)   \rUnpacking objects:  12% (2/16)   \rUnpacking objects:  18% (3/16)   \rUnpacking objects:  25% (4/16)   \rUnpacking objects:  31% (5/16)   \rUnpacking objects:  37% (6/16)   \rUnpacking objects:  43% (7/16)   \rUnpacking objects:  50% (8/16)   \rUnpacking objects:  56% (9/16)   \rUnpacking objects:  62% (10/16)   \rUnpacking objects:  68% (11/16)   \rUnpacking objects:  75% (12/16)   \rUnpacking objects:  81% (13/16)   \rUnpacking objects:  87% (14/16)   \rUnpacking objects:  93% (15/16)   \rUnpacking objects: 100% (16/16)   \rUnpacking objects: 100% (16/16), done.\n",
            "/content/intro2dl/data/intro2dl/data/intro2dl/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zge93uqARUlY"
      },
      "source": [
        "## Regression with Boston House Price dataset\n",
        "\n",
        "### 1. Load data\n",
        "\n",
        "In this notebook, we are going to use the [**Boston house price dataset** dataset](https://archive.ics.uci.edu/ml/datasets/Housing). \n",
        "\n",
        "The dataset describes 13 numerical properties of houses in Boston suburbs and is concerned with modeling the price of houses in those suburbs in thousands of dollars. As such, this is a regression predictive modeling problem. Input attributes include things like crime rate, proportion of nonretail business acres, chemical concentrations and more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7faz-YVRRYW"
      },
      "source": [
        "from pandas import read_csv\n",
        "\n",
        "# load dataset\n",
        "dataframe = read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X79HF3ZdSBjF"
      },
      "source": [
        "### 2. Define Base Model\n",
        "\n",
        "Create a Keras model with 1 hidden layer (size = input layer size).\n",
        "\n",
        "We should define a `baseline_model()` funtion that will create the model, compile it and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAYRmljaSBcE"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# define baseline model\n",
        "def baseline_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(13, input_dim=13, activation=\"relu\"))\n",
        "\tmodel.add(Dense(1, activation=\"linear\"))\n",
        " \n",
        "\tmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
        "\n",
        "\treturn model"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtuQxqLWSdq6"
      },
      "source": [
        "### 3. Evaluate baseline model\n",
        "\n",
        "Evaluate the model using stratified cross validation in the scikit-learn framework. Number of splits should be 10. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPRh8GcySXRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627dd944-0211-4425-fb8b-2f2a886611c4"
      },
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# evaluate model\n",
        "estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10, random_state=42)\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
        "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Baseline: -86.77 (67.96) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-wSXuEETZP-"
      },
      "source": [
        "## Lift Performance By Standardizing The Dataset\n",
        "Standardizing the dataset referes to transforming all datapoints values to the range of 0 to 1. This is done using `scikit-learn`'s StandardScaler. We will also build a pipeline which will call the function creating the model, then compile and training it. The last step is to evaluate the performance of the model using cross-validation. This will show us whether standardizing a dataset betters the performance of a deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWL2SwgiThyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d7f2be-5df0-497b-ad06-2f32f11f480a"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# evaluate baseline model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)))\n",
        "\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Standarized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standarized: -64.68 (18.66) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KqRL6cUTwvs"
      },
      "source": [
        "## Tune The Neural Network Topology\n",
        "We can alter the architecture of the hidden layers of the neural network to observe changes in the results we get."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rAzSWHCbvAt"
      },
      "source": [
        "### Evaluate a wider network\n",
        "A wider network is a network where the hidden layer has more neurons than it previously had. Let's create a network which has 25 neurons in the hidden layer instead of thirteen. So, almost double."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On0KRd4TTrmD"
      },
      "source": [
        "def wider_model():\n",
        "\t# FILL BLANKS\n",
        "\n",
        "\n",
        "  model= Sequential()\n",
        "  model.add(Dense(25, activation=\"relu\", input_dim=13))\n",
        "  model.add(Dense(1,  activation= \"linear\"))\n",
        "  model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")  \n",
        "  return model"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3zhESl1cb16"
      },
      "source": [
        "Next up, standardizing the dataset and using the pipeline to build, compile and train the model to get the score of this wider network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJR72K2IUDb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292321b1-ac2c-4879-d85b-5abe21a3d3bb"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# evaluate baseline model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=10, batch_size=5, verbose=0)))\n",
        "\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wider: -36.18 (11.69) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJFKgHmjT0TL"
      },
      "source": [
        "### Evaluate a deeper network\n",
        "A deeper network is a network which has more hidden layers than the previous baseline network. Let's add another hidden layer for a total of two hidden layers with 13 neurons each and check the results we get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb_3D9InTz70"
      },
      "source": [
        "def deeper_model():\n",
        "\t# FILL BLANKS\n",
        "\n",
        "\n",
        "  model= Sequential()\n",
        "  model.add(Dense(25, activation=\"relu\", input_dim=13))\n",
        "  model.add(Dense(10, activation=\"relu\"))\n",
        "  model.add(Dense(1,  activation= \"linear\"))\n",
        "  model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")  \n",
        "  return model"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNJwqfKLdpl3"
      },
      "source": [
        "Standardizing the dataset and using the pipeline to build, compile and train the model to get the score of this wider network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgg0TGhRURIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2ae783-bbff-4a9e-eff7-87b2889ba54b"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# evaluate baseline model with standardized dataset\n",
        "# FILL BLANKS\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=deeper_model, epochs=10, batch_size=5, verbose=0)))\n",
        "\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"deeper: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deeper: -22.35 (7.31) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}