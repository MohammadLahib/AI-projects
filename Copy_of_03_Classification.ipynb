{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 03_Classification",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2-lh/moh2-lh/blob/main/Copy_of_03_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCMaK16lt_mc"
      },
      "source": [
        "© 2021 Zaka AI, Inc. All Rights Reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwUowJNBDf3E"
      },
      "source": [
        "#Binary and Multi-class classification\n",
        "**Objective:** This notebook is comprised of two independent exercises: a multi-classication with Iris flower data and a binary classification with sonar data. The objective of the first exercise is to prepare data for a multiclassification model and training it. For the second, We will train and evaluate a binary classification model and learn how to apply standardization on a dataset and create a pipeline for evaluation of models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmJvLAn6yMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae65a54d-d8f5-401f-8f1c-2c433fe9dd8c"
      },
      "source": [
        "# clone git repo\n",
        "!git clone https://github.com/zaka-ai/intro2dl.git\n",
        "\n",
        "# change directory\n",
        "%cd intro2dl/data/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'intro2dl'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/16)\u001b[K\rremote: Counting objects:  12% (2/16)\u001b[K\rremote: Counting objects:  18% (3/16)\u001b[K\rremote: Counting objects:  25% (4/16)\u001b[K\rremote: Counting objects:  31% (5/16)\u001b[K\rremote: Counting objects:  37% (6/16)\u001b[K\rremote: Counting objects:  43% (7/16)\u001b[K\rremote: Counting objects:  50% (8/16)\u001b[K\rremote: Counting objects:  56% (9/16)\u001b[K\rremote: Counting objects:  62% (10/16)\u001b[K\rremote: Counting objects:  68% (11/16)\u001b[K\rremote: Counting objects:  75% (12/16)\u001b[K\rremote: Counting objects:  81% (13/16)\u001b[K\rremote: Counting objects:  87% (14/16)\u001b[K\rremote: Counting objects:  93% (15/16)\u001b[K\rremote: Counting objects: 100% (16/16)\u001b[K\rremote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects:   6% (1/15)\u001b[K\rremote: Compressing objects:  13% (2/15)\u001b[K\rremote: Compressing objects:  20% (3/15)\u001b[K\rremote: Compressing objects:  26% (4/15)\u001b[K\rremote: Compressing objects:  33% (5/15)\u001b[K\rremote: Compressing objects:  40% (6/15)\u001b[K\rremote: Compressing objects:  46% (7/15)\u001b[K\rremote: Compressing objects:  53% (8/15)\u001b[K\rremote: Compressing objects:  60% (9/15)\u001b[K\rremote: Compressing objects:  66% (10/15)\u001b[K\rremote: Compressing objects:  73% (11/15)\u001b[K\rremote: Compressing objects:  80% (12/15)\u001b[K\rremote: Compressing objects:  86% (13/15)\u001b[K\rremote: Compressing objects:  93% (14/15)\u001b[K\rremote: Compressing objects: 100% (15/15)\u001b[K\rremote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "Unpacking objects:   6% (1/16)   \rUnpacking objects:  12% (2/16)   \rUnpacking objects:  18% (3/16)   \rUnpacking objects:  25% (4/16)   \rUnpacking objects:  31% (5/16)   \rUnpacking objects:  37% (6/16)   \rUnpacking objects:  43% (7/16)   \rUnpacking objects:  50% (8/16)   \rUnpacking objects:  56% (9/16)   \rUnpacking objects:  62% (10/16)   \rremote: Total 16 (delta 1), reused 7 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  68% (11/16)   \rUnpacking objects:  75% (12/16)   \rUnpacking objects:  81% (13/16)   \rUnpacking objects:  87% (14/16)   \rUnpacking objects:  93% (15/16)   \rUnpacking objects: 100% (16/16)   \rUnpacking objects: 100% (16/16), done.\n",
            "/content/intro2dl/data/intro2dl/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV7goI5hEJ5G"
      },
      "source": [
        "## Multi-class classification with Iris Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3PehS5h75-e"
      },
      "source": [
        "### 1. Load data\n",
        "\n",
        "In this notebook, we are going to use the [**Iris flower** dataset](https://archive.ics.uci.edu/ml/datasets/Iris). This is another standard machine learning dataset from the UCI Machine Learning repository. Each instance describes the properties of an observed flower measurements and the output variable is specific iris species.\n",
        "\n",
        "This is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species.\n",
        "\n",
        "The variables can be summarized as follows:\n",
        "\n",
        "**Input Variables (X):**\n",
        "\n",
        "\n",
        "1. Sepal length in cm\n",
        "2. Sepal width in cm\n",
        "3. Petal length in cm\n",
        "4. Petal width in cm\n",
        "\n",
        "**Output Variable (Y):**\n",
        "\n",
        "*   Class:\n",
        " - Iris Setosa\n",
        " - Iris Versicolour\n",
        " - Iris Virginica\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6493eiB-71lx"
      },
      "source": [
        "from pandas import read_csv\n",
        "\n",
        "# load dataset\n",
        "dataframe = read_csv(\"iris.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split X and Y features\n",
        "X = dataset[:,0:4].astype(float)\n",
        "Y = dataset[:,4]\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQbqVaPU914x"
      },
      "source": [
        "### 2. Encode the output variable\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME2DAz4k94SB"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# encode class values as integers\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "\n",
        "encoder_y =  encoder.transform(Y)\n",
        "\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "\n",
        "dummy_y = to_categorical(encoder_y)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-4v8_6v-BDj"
      },
      "source": [
        "### 3. Define Keras Model\n",
        "\n",
        "Create a Keras Sequential model that has 1 hidden layers, with the `relu` activation function. \n",
        "\n",
        "We should define a `create_model()` funtion that will create the model, compile it and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9GhvIx0973_"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# define baseline model\n",
        "def create_model():\n",
        "\t# create model\n",
        "\n",
        "  model= Sequential()\n",
        "  model.add(Dense(12, activation='relu', input_dim=4))\n",
        "  model.add(Dense(3, activation= 'softmax'))\n",
        "\t\n",
        "\t# Compile model\n",
        "\n",
        "  model.compile(optimizer= 'sgd', loss=\"categorical_crossentropy\", metrics= ['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbapy2_R-ohc"
      },
      "source": [
        "### 4. Train Model\n",
        "\n",
        "Let's train the model for 20 epochs with batch size equals to 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivmHGOKN-t4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45a1353-b96c-4fdd-ed4b-9852ad44e399"
      },
      "source": [
        "# FILL BLANKS\n",
        "# train model\n",
        "\n",
        "model =  create_model()\n",
        "\n",
        "model.fit(X, dummy_y, epochs=20, batch_size= 5 )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 1s 1ms/step - loss: 1.2238 - accuracy: 0.3467 \n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.9007 - accuracy: 0.6667\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.8380 - accuracy: 0.6667\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.7744 - accuracy: 0.6667\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.7095 - accuracy: 0.6733\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.6414 - accuracy: 0.7533\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.5863 - accuracy: 0.8200\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.5379 - accuracy: 0.8733\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.4957 - accuracy: 0.9133\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.4652 - accuracy: 0.9333\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.4367 - accuracy: 0.9067\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.4108 - accuracy: 0.9400\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.3871 - accuracy: 0.9733\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.3755 - accuracy: 0.9467\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.3568 - accuracy: 0.9533\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.3448 - accuracy: 0.9533\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.9533\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.3131 - accuracy: 0.9533\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.3060 - accuracy: 0.9600\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 0.2922 - accuracy: 0.9733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd4adc41250>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nle1n0RuESpH"
      },
      "source": [
        "## Binary Classification with Sonar Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1AAzP8rDMhO"
      },
      "source": [
        "### 1. Load dataset\n",
        "\n",
        "The dataset we will use in this tutorial is the [Sonar dataset](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)).\n",
        "\n",
        "This is a dataset that describes sonar chirp returns bouncing off different services. The 60 input variables are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders.\n",
        "\n",
        "It is a well-understood dataset. All of the variables are continuous and generally in the range of 0 to 1. The output variable is a string “M” for mine and “R” for rock, which will need to be converted to integers 1 and 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbz99VsDCssb"
      },
      "source": [
        "# Binary Classification with Sonar Dataset: Baseline\n",
        "\n",
        "from pandas import read_csv\n",
        "# load dataset\n",
        "dataframe = read_csv(\"sonar.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFfJu1sXLl2X"
      },
      "source": [
        "### 2. Encode output variable\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzI15A8ELoYC"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hhB461HLtjw"
      },
      "source": [
        "### 3. Define Keras Model\n",
        "\n",
        "Create a Keras model with 1 hidden layer of size 60 and 1 output layer. The layers should have a 'normal' initialization of weights.\n",
        "\n",
        "Compile the model with adam optimizer.\n",
        "\n",
        "We should define a `baseline_model()` funtion that will create the model, compile it and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lVz94h7LwYl"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def create_baseline():\n",
        "\t# FILL BLANKS\n",
        "\t# create model\n",
        "\n",
        "  model =  Sequential()\n",
        "  model.add(Dense(60, input_dim= 60, activation='relu', kernel_initializer= \"normal\"))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        " #compile the model\n",
        " \n",
        "  model.compile(optimizer='adam', loss= 'binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "turVjiiWMR_4"
      },
      "source": [
        "### 4. Evaluate model\n",
        "\n",
        "Evaluate the model using stratified cross validation in the scikit-learn framework. Number of splits should be 10. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JND-sahHMRNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d105b880-253d-4927-cc60-57f37bc38c71"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# evaluate model with dataset\n",
        "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=15)\n",
        "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: 83.67% (7.83%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oKP2BnHM4KI"
      },
      "source": [
        "## Apply Standardization on Dataset\n",
        "\n",
        "An effective data preparation scheme for tabular data when building neural network models is **standardization**. This is where the data is rescaled such that the mean value for each attribute is 0 and the standard deviation is 1. This preserves Gaussian and Gaussian-like distributions whilst normalizing the central tendencies for each attribute.\n",
        "\n",
        "We can use scikit-learn to perform the standardization of our Sonar dataset using the `StandardScaler` class.\n",
        "\n",
        "## Create a pipeline\n",
        "\n",
        "The Scikit-learn pipeline is a wrapper that executes one or more models within a pass of the cross-validation procedure. Here, we can define a pipeline with the StandardScaler followed by our neural network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIsFIrnjM86W"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# evaluate baseline model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
        "\n",
        "pipeline = Pipeline(estimators)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZlS0W8cNeYx"
      },
      "source": [
        "### Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbodsmh2Ngbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cecdb4-87dc-4166-dd4c-9530d8b0bdfb"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=15)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standardized: 87.48% (5.40%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}